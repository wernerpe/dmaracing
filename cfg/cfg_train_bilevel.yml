seed: 1
clip_observations: 1000.0
clip_actions: 4.0

algorithm:
  clip_param: 0.2
  entropy_coef: 0.02
  num_learning_epochs: 15
  num_mini_batches: 6 # this is per agent
  learning_rate: 5.e-5
  schedule: adaptive # could be adaptive, linear or fixed
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.008
  value_loss_coef: 1.0
  max_grad_norm: 0.5
  use_clipped_value_loss: True

runner:
  policy_class_name: MAActorCritic
  algorithm_class_name: MAPPO
  num_steps_per_env_hl: 20  # 64 # per iteration
  num_steps_per_env_ll: 60
  max_iterations: 2400  # 1200 # 500 # number of policy updates
  population_update_interval: 100 #number of iterations before the opponent policy is updated
  # logging
  save_interval: 50 # check for potential saves every this many iterations
  experiment_name: 'tri_single_blr_hierarchical'
  run_name: ''
  # load and resume
  resume: False
  load_run: -1 # -1 = last run
  checkpoint: -1 # -1 = last saved model
  resume_path: None # updated from load_run and chkpt

policy: 
  actor_hidden_dims: [256, 128]
  critic_hidden_dims: [256, 128]
  encoder_hidden_dims: [64]
  activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid
  init_noise_std: 0.3  # 1.0
  attentive: True
  encoder_type: attention2  # attention{1, 2}
  