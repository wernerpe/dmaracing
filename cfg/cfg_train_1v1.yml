seed: 1
clip_observations: 1000.0
clip_actions: 4.0

track_cfgs: True

algorithm:
  clip_param: 0.2
  entropy_coef: 0.005
  num_learning_epochs: 15  # 5
  num_mini_batches: 6 # this is per agent
  learning_rate: 5.e-5
  schedule: adaptive # could be adaptive, linear or fixed
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.008
  value_loss_coef: 1.0
  max_grad_norm: 0.5
  use_clipped_value_loss: True

runner:
  policy_class_name: MAActorCritic #MultiTeamCMAAC 
  algorithm_class_name: IMAPPO #JRMAPPO 
  num_steps_per_env: 64  # 32 # per iteration
  max_iterations: 50000 # number of policy updates
  population_update_interval: 100  # 40 #number of iterations before the opponent policy is updated
  # logging
  save_interval: 80 # check for potential saves every this many iterations
  eval_interval: 40
  experiment_name: 'tri_1v1'  # '2v2'
  run_name: ''
  # load and resume
  resume: False
  load_run: -1 # -1 = last run
  checkpoint: -1 # -1 = last saved model
  resume_path: None # updated from load_run and chkpt

policy: 
  actor_hidden_dims: [256, 128]
  critic_hidden_dims: [256, 128]
  encoder_hidden_dims: [64]
  activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid
  init_noise_std: 1.0
  attentive: True
  encoder_type: attention2  # attention{1, 2}
  teamsize: 1
  numteams: 2
  numcritics: 1
  n_ado_polices: 5
  max_num_old_models: 40
  kl_save_threshold: 1.5
  predict_stddev: False