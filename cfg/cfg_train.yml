seed: 1
clip_observations: 1000.0
clip_actions: 4.0

track_cfgs: True

algorithm:
  clip_param: 0.2
  entropy_coef: 0.005
  num_learning_epochs: 15  # 15  # 5
  num_mini_batches: 6 # this is per agent
  learning_rate: 5.e-5
  schedule: adaptive # could be adaptive, linear or fixed
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.008
  value_loss_coef: 1.0
  max_grad_norm: 10.  # 0.5
  use_clipped_value_loss: True

runner:
  policy_class_name: MultiTeamCMAAC 
  algorithm_class_name: JRMAPPO 
  num_steps_per_env: 128  # 64  # 32 # per iteration
  max_iterations: 50000 # number of policy updates
  population_update_interval: 40  # 10  # 40 #number of iterations before the opponent policy is updated
  # logging
  save_interval: 1000 # check for potential saves every this many iterations
  eval_interval: 20
  experiment_name: '2v2_tuning'  # '1v1_tuning'  # '2v2'
  run_name: ''
  # load and resume
  resume: False
  load_run: -1 # -1 = last run
  checkpoint: -1 # -1 = last saved model
  resume_path: None # updated from load_run and chkpt

policy: 
  actor_hidden_dims: [512, 256, 32]  # [512, 256, 32]
  critic_hidden_dims: [512, 256, 32]  # [512, 256, 32]
  encoder_hidden_dims: [32]  # [128, 32]  # [256, 256]
  activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid
  init_noise_std: 0.5
  attentive: True
  encoder_type: attention4  # attention{1, 2}
  teamsize: 2  #2
  numteams: 2
  numcritics: 5
  max_num_old_models: 20  # 5
  kl_save_threshold: 1.5